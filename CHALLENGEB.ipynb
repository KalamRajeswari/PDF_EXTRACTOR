{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81b3d629-7248-4c44-9b61-79debc84242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extraction complete. Output saved to output_result.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class HeadingExtractor:\n",
    "    def __init__(self):\n",
    "        self.heading_keywords = [\n",
    "            \"introduction\", \"abstract\", \"conclusion\", \"references\",\n",
    "            \"methodology\", \"results\", \"discussion\", \"background\",\n",
    "            \"chapter\", \"section\", \"appendix\"\n",
    "        ]\n",
    "        self.min_font_occurrences = 3\n",
    "        self.max_heading_words = 12\n",
    "\n",
    "    def extract_from_pdf(self, pdf_path):\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            toc = doc.get_toc()\n",
    "            if toc and len(toc) > 2:\n",
    "                return self._extract_from_toc(doc, toc)\n",
    "            return self._extract_with_nlp(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from PDF: {e}\")\n",
    "            return {\"title\": \"Unknown\", \"outline\": []}\n",
    "\n",
    "    def _extract_from_toc(self, doc, toc):\n",
    "        title = doc.metadata.get(\"title\", \"\")\n",
    "        outline = []\n",
    "        for item in toc:\n",
    "            level, heading_text, page = item\n",
    "            if level <= 3:\n",
    "                outline.append({\n",
    "                    \"level\": f\"H{level}\",\n",
    "                    \"text\": heading_text,\n",
    "                    \"page\": page-1\n",
    "                })\n",
    "        if not title and outline:\n",
    "            title = outline[0][\"text\"]\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"outline\": sorted(outline, key=lambda x: x[\"page\"])\n",
    "        }\n",
    "\n",
    "    def _extract_with_nlp(self, doc):\n",
    "        font_stats = self._analyze_fonts(doc)\n",
    "        font_stats[\"spans\"] = self._merge_close_spans(font_stats[\"spans\"])\n",
    "        candidates = self._extract_candidate_headings(doc, font_stats)\n",
    "        headings = self._classify_headings_with_nlp(candidates)\n",
    "        outline = self._assign_heading_levels(headings)\n",
    "        outline = self._deduplicate_outline(outline)\n",
    "\n",
    "        outline.sort(key=lambda x: x[\"page\"])\n",
    "\n",
    "        title = doc.metadata.get(\"title\", \"\")\n",
    "        if not title and outline:\n",
    "            title = outline[0][\"text\"]\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"outline\": outline\n",
    "        }\n",
    "\n",
    "    def _analyze_fonts(self, doc):\n",
    "        fonts = defaultdict(int)\n",
    "        font_details = []\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for block in blocks:\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        size = round(span.get(\"size\", 0), 1)\n",
    "                        is_bold = span.get(\"flags\", 0) & 2 > 0\n",
    "                        text = span.get(\"text\", \"\").strip()\n",
    "                        if not text:\n",
    "                            continue\n",
    "                        fonts[size] += 1\n",
    "                        font_details.append({\n",
    "                            \"text\": text,\n",
    "                            \"size\": size,\n",
    "                            \"bold\": is_bold,\n",
    "                            \"page\": page_num ,\n",
    "                            \"y_pos\": span.get(\"bbox\")[1],\n",
    "                            \"bbox\": span.get(\"bbox\")\n",
    "                        })\n",
    "        common_fonts = sorted(\n",
    "            [(size, count) for size, count in fonts.items()\n",
    "             if count >= self.min_font_occurrences],\n",
    "            key=lambda x: x[0], reverse=True\n",
    "        )\n",
    "        return {\"common_fonts\": common_fonts, \"spans\": font_details}\n",
    "\n",
    "    def _merge_close_spans(self, spans, y_threshold=1.5):\n",
    "        merged = []\n",
    "        current = None\n",
    "        for span in sorted(spans, key=lambda x: (x['page'], x['y_pos'])):\n",
    "            if current and abs(current['y_pos'] - span['y_pos']) < y_threshold and current['page'] == span['page']:\n",
    "                current['text'] += ' ' + span['text']\n",
    "            else:\n",
    "                if current:\n",
    "                    merged.append(current)\n",
    "                current = span.copy()\n",
    "        if current:\n",
    "            merged.append(current)\n",
    "        return merged\n",
    "\n",
    "    def _extract_candidate_headings(self, doc, font_stats):\n",
    "        candidates = []\n",
    "        common_fonts = font_stats[\"common_fonts\"]\n",
    "        spans = font_stats[\"spans\"]\n",
    "        spans_by_page = defaultdict(list)\n",
    "        for span in spans:\n",
    "            spans_by_page[span[\"page\"]].append(span)\n",
    "        for span in spans:\n",
    "            text = span[\"text\"]\n",
    "            size = span[\"size\"]\n",
    "            is_bold = span[\"bold\"]\n",
    "            page = span[\"page\"]\n",
    "            if len(text.split()) > self.max_heading_words:\n",
    "                continue\n",
    "            features = {\n",
    "                \"font_size\": size,\n",
    "                \"is_bold\": is_bold,\n",
    "                \"word_count\": len(text.split()),\n",
    "                \"has_number_prefix\": bool(re.match(r'^\\d+(\\.\\d+)*\\.?\\s', text)),\n",
    "                \"is_all_caps\": text.isupper(),\n",
    "                \"ends_with_colon\": text.endswith(':'),\n",
    "                \"has_heading_keyword\": any(keyword in text.lower() for keyword in self.heading_keywords),\n",
    "                \"at_page_top\": self._is_at_page_top(span, spans_by_page[page]),\n",
    "                \"standalone_line\": self._is_standalone(span, spans_by_page[page])\n",
    "            }\n",
    "            score = self._score_candidate(features, common_fonts)\n",
    "            if score > 0.5:\n",
    "                candidates.append({\n",
    "                    \"text\": text,\n",
    "                    \"page\": page,\n",
    "                    \"features\": features,\n",
    "                    \"score\": score,\n",
    "                    \"size\": size,\n",
    "                    \"is_bold\": is_bold,\n",
    "                    \"y_pos\": span[\"y_pos\"],\n",
    "                    \"bbox\": span[\"bbox\"],\n",
    "                })\n",
    "        return candidates\n",
    "\n",
    "    def _is_at_page_top(self, span, page_spans):\n",
    "        if not page_spans:\n",
    "            return False\n",
    "        sorted_spans = sorted(page_spans, key=lambda x: x[\"y_pos\"])\n",
    "        return span[\"y_pos\"] <= sorted_spans[0][\"y_pos\"] + 0.15 * (sorted_spans[-1][\"y_pos\"] - sorted_spans[0][\"y_pos\"])\n",
    "\n",
    "    def _is_standalone(self, span, page_spans):\n",
    "        bbox = span[\"bbox\"]\n",
    "        for other in page_spans:\n",
    "            if other == span:\n",
    "                continue\n",
    "            y_overlap = max(0, min(bbox[3], other[\"bbox\"][3]) - max(bbox[1], other[\"bbox\"][1]))\n",
    "            if y_overlap > 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _score_candidate(self, features, common_fonts):\n",
    "        score = 0\n",
    "        for i, (font_size, _) in enumerate(common_fonts[:3]):\n",
    "            if abs(features[\"font_size\"] - font_size) < 0.5:\n",
    "                score += 0.3 - (i * 0.1)\n",
    "                break\n",
    "        if features[\"is_bold\"]:\n",
    "            score += 0.2\n",
    "        if features[\"has_number_prefix\"]:\n",
    "            score += 0.3\n",
    "        if features[\"is_all_caps\"]:\n",
    "            score += 0.15\n",
    "        if features[\"ends_with_colon\"]:\n",
    "            score += 0.15\n",
    "        if features[\"has_heading_keyword\"]:\n",
    "            score += 0.2\n",
    "        if features[\"at_page_top\"]:\n",
    "            score += 0.25\n",
    "        if features[\"standalone_line\"]:\n",
    "            score += 0.2\n",
    "        if features[\"word_count\"] > 8:\n",
    "            score -= 0.1 * (features[\"word_count\"] - 8)\n",
    "        return min(1.0, max(0.0, score))\n",
    "\n",
    "    def _classify_headings_with_nlp(self, candidates):\n",
    "        if not candidates:\n",
    "            return []\n",
    "        texts = [c[\"text\"] for c in candidates]\n",
    "        docs = list(nlp.pipe(texts, disable=[\"ner\"]))\n",
    "        for i, (candidate, doc) in enumerate(zip(candidates, docs)):\n",
    "            has_verb = any(token.pos_ == \"VERB\" for token in doc)\n",
    "            pos_pattern = \" \".join([token.pos_ for token in doc])\n",
    "            common_heading_patterns = [\n",
    "                \"^(DET )?(ADJ )*(NOUN|PROPN)\",\n",
    "                \"^NUM\",\n",
    "                \"^(VERB|AUX)\",\n",
    "                \"^ADV ADJ\"\n",
    "            ]\n",
    "            matches_pattern = any(re.search(pattern, pos_pattern) for pattern in common_heading_patterns)\n",
    "            if matches_pattern:\n",
    "                candidates[i][\"score\"] += 0.2\n",
    "            if has_verb and len(doc) > 5:\n",
    "                candidates[i][\"score\"] -= 0.25\n",
    "            if '.' in candidate[\"text\"] or len(candidate[\"text\"].split()) > 12:\n",
    "                candidates[i][\"score\"] -= 0.3\n",
    "        return [c for c in candidates if c[\"score\"] >= 0.6]\n",
    "\n",
    "    def _assign_heading_levels(self, headings):\n",
    "        if not headings:\n",
    "            return []\n",
    "        sorted_headings = sorted(headings, key=lambda x: x[\"score\"], reverse=True)\n",
    "        sizes = [h[\"size\"] for h in sorted_headings]\n",
    "        size_counts = defaultdict(int)\n",
    "        for size in sizes:\n",
    "            size_counts[size] += 1\n",
    "        common_sizes = sorted(size_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        size_to_level = {}\n",
    "        for i, (size, _) in enumerate(common_sizes[:3]):\n",
    "            size_to_level[size] = f\"H{i+1}\"\n",
    "        default_level = \"H3\"\n",
    "        outline = []\n",
    "        for heading in sorted_headings:\n",
    "            level = size_to_level.get(heading[\"size\"], default_level)\n",
    "            if heading[\"score\"] < 0.75 and level == \"H3\":\n",
    "                continue\n",
    "            outline.append({\n",
    "                \"level\": level,\n",
    "                \"text\": heading[\"text\"],\n",
    "                \"page\": heading[\"page\"],\n",
    "                \"y_pos\": heading[\"y_pos\"]\n",
    "            })\n",
    "        return outline\n",
    "\n",
    "    def _deduplicate_outline(self, outline):\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for item in outline:\n",
    "            key = (item[\"text\"].strip().lower(), item[\"page\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                result.append(item)\n",
    "        return result\n",
    "\n",
    "def extract_refined_text(pdf_path, page_number, heading_y_pos, max_sentences=3):\n",
    "    \"\"\"Extracts up to max_sentences of text below the heading position on the specified page.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_number]\n",
    "    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "    candidate_blocks = [b for b in blocks if b.get(\"bbox\") and b[\"bbox\"][1] > heading_y_pos + 2]\n",
    "    candidate_blocks = sorted(candidate_blocks, key=lambda b: b[\"bbox\"][1])\n",
    "\n",
    "    accumulated_text = \"\"\n",
    "    sentence_count = 0\n",
    "\n",
    "    for block in candidate_blocks:\n",
    "        block_text = \"\"\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                block_text += span.get(\"text\", \"\") + \" \"\n",
    "        block_text = block_text.strip()\n",
    "\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', block_text)\n",
    "\n",
    "        for sent in sentences:\n",
    "            if sentence_count < max_sentences:\n",
    "                accumulated_text += sent + \" \"\n",
    "                sentence_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if sentence_count >= max_sentences:\n",
    "            break\n",
    "\n",
    "    accumulated_text = accumulated_text.strip()\n",
    "    if sentence_count >= max_sentences:\n",
    "        accumulated_text += \"...\"\n",
    "\n",
    "    return accumulated_text if accumulated_text else \"\"\n",
    "\n",
    "def is_generic_heading(title):\n",
    "    generic_titles = {\"introduction\", \"summary\", \"overview\", \"contents\"}\n",
    "    return title.strip().lower() in generic_titles\n",
    "\n",
    "def section_score(section):\n",
    "    title = section[\"section_title\"].lower()\n",
    "    score = 0\n",
    "    keywords = [\"guide\", \"adventure\", \"tips\", \"experience\", \"travel\", \"nightlife\", \"culinary\", \"history\", \"restaurants\", \"hotels\", \"things to do\"]\n",
    "    for keyword in keywords:\n",
    "        if keyword in title:\n",
    "            score += 3\n",
    "    if len(title.split()) > 3:\n",
    "        score += 1\n",
    "    return score\n",
    "\n",
    "def main():\n",
    "    pdf_folder = \"input_pdfs\"  # Your PDFs folder\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    persona = \"Travel Planner\"\n",
    "    job_to_be_done = \"Plan a trip of 4 days for a group of 10 college friends.\"\n",
    "    processing_timestamp = datetime.now().isoformat()\n",
    "\n",
    "    extractor = HeadingExtractor()\n",
    "    all_sections = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        full_path = os.path.join(pdf_folder, pdf_file)\n",
    "        result = extractor.extract_from_pdf(full_path)\n",
    "        for section in result.get(\"outline\", []):\n",
    "            all_sections.append({\n",
    "                \"document\": pdf_file,\n",
    "                \"section_title\": section[\"text\"],\n",
    "                \"page_number\": section[\"page\"],\n",
    "                \"y_pos\": section.get(\"y_pos\", 0)\n",
    "            })\n",
    "\n",
    "    filtered_sections = [s for s in all_sections if not is_generic_heading(s[\"section_title\"])]\n",
    "\n",
    "    if len(filtered_sections) < 5:\n",
    "        intro_sections = [s for s in all_sections if is_generic_heading(s[\"section_title\"])]\n",
    "        intro_by_doc = {}\n",
    "        for s in intro_sections:\n",
    "            if s[\"document\"] not in intro_by_doc:\n",
    "                intro_by_doc[s[\"document\"]] = s\n",
    "        filtered_sections.extend(intro_by_doc.values())\n",
    "\n",
    "    all_sections = filtered_sections\n",
    "\n",
    "    all_sections.sort(key=lambda s: (section_score(s), -s[\"page_number\"]), reverse=True)\n",
    "\n",
    "    seen_titles = set()\n",
    "    final_sections = []\n",
    "    for s in all_sections:\n",
    "        title_lower = s[\"section_title\"].strip().lower()\n",
    "        if title_lower not in seen_titles:\n",
    "            seen_titles.add(title_lower)\n",
    "            final_sections.append(s)\n",
    "        if len(final_sections) >= 5:\n",
    "            break\n",
    "\n",
    "    final_subsections = []\n",
    "    for s in final_sections:\n",
    "        refined_text = extract_refined_text(\n",
    "            os.path.join(pdf_folder, s[\"document\"]),\n",
    "            s[\"page_number\"],\n",
    "            s.get(\"y_pos\", 0)\n",
    "        )\n",
    "        final_subsections.append({\n",
    "            \"document\": s[\"document\"],\n",
    "            \"refined_text\": refined_text,\n",
    "            \"page_number\": s[\"page_number\"]\n",
    "        })\n",
    "\n",
    "    output = {\n",
    "        \"metadata\": {\n",
    "            \"input_documents\": pdf_files,\n",
    "            \"persona\": persona,\n",
    "            \"job_to_be_done\": job_to_be_done,\n",
    "            \"processing_timestamp\": processing_timestamp\n",
    "        },\n",
    "        \"extracted_sections\": [\n",
    "            {\n",
    "                \"document\": s[\"document\"],\n",
    "                \"section_title\": s[\"section_title\"],\n",
    "                \"importance_rank\": i + 1,\n",
    "                \"page_number\": s[\"page_number\"]\n",
    "            } for i, s in enumerate(final_sections)\n",
    "        ],\n",
    "        \"subsection_analysis\": final_subsections\n",
    "    }\n",
    "\n",
    "    with open(\"output_result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"‚úÖ Extraction complete. Output saved to output_result.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f7d56b-f380-41a3-b72f-d1b971cc188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extraction complete. Output saved to output_result.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class HeadingExtractor:\n",
    "    def __init__(self):\n",
    "        self.min_font_occurrences = 3\n",
    "        self.max_heading_words = 12\n",
    "\n",
    "    def extract_from_pdf(self, pdf_path):\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            toc = doc.get_toc()\n",
    "            if toc and len(toc) > 2:\n",
    "                # Use TOC if available\n",
    "                return self._extract_from_toc(doc, toc)\n",
    "            return self._extract_with_nlp(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from PDF: {e}\")\n",
    "            return {\"title\": \"Unknown\", \"outline\": []}\n",
    "\n",
    "    def _extract_from_toc(self, doc, toc):\n",
    "        title = doc.metadata.get(\"title\", \"\")\n",
    "        outline = []\n",
    "        for item in toc:\n",
    "            level, heading_text, page = item\n",
    "            if level <= 3:\n",
    "                outline.append({\n",
    "                    \"level\": f\"H{level}\",\n",
    "                    \"text\": heading_text,\n",
    "                    \"page\": page  # usually 1-based from TOC\n",
    "                })\n",
    "        if not title and outline:\n",
    "            title = outline[0][\"text\"]\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"outline\": sorted(outline, key=lambda x: x[\"page\"])\n",
    "        }\n",
    "\n",
    "    def _extract_with_nlp(self, doc):\n",
    "        font_stats = self._analyze_fonts(doc)\n",
    "        font_stats[\"spans\"] = self._merge_close_spans(font_stats[\"spans\"])\n",
    "        candidates = self._extract_candidate_headings(doc, font_stats)\n",
    "        headings = self._classify_headings_with_nlp(candidates)\n",
    "        outline = self._assign_heading_levels(headings)\n",
    "        outline = self._deduplicate_outline(outline)\n",
    "\n",
    "        # Convert zero-based page index to 1-based\n",
    "        for item in outline:\n",
    "            item[\"page\"] += 1\n",
    "\n",
    "        outline.sort(key=lambda x: x[\"page\"])\n",
    "\n",
    "        title = doc.metadata.get(\"title\", \"\")\n",
    "        if not title and outline:\n",
    "            title = outline[0][\"text\"]\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"outline\": outline\n",
    "        }\n",
    "\n",
    "    def _analyze_fonts(self, doc):\n",
    "        fonts = defaultdict(int)\n",
    "        font_details = []\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for block in blocks:\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        size = round(span.get(\"size\", 0), 1)\n",
    "                        is_bold = span.get(\"flags\", 0) & 2 > 0\n",
    "                        text = span.get(\"text\", \"\").strip()\n",
    "                        if not text:\n",
    "                            continue\n",
    "                        fonts[size] += 1\n",
    "                        font_details.append({\n",
    "                            \"text\": text,\n",
    "                            \"size\": size,\n",
    "                            \"bold\": is_bold,\n",
    "                            \"page\": page_num,\n",
    "                            \"y_pos\": span.get(\"bbox\")[1],\n",
    "                            \"bbox\": span.get(\"bbox\")\n",
    "                        })\n",
    "        common_fonts = sorted(\n",
    "            [(size, count) for size, count in fonts.items()\n",
    "             if count >= self.min_font_occurrences],\n",
    "            key=lambda x: x[0], reverse=True\n",
    "        )\n",
    "        return {\"common_fonts\": common_fonts, \"spans\": font_details}\n",
    "\n",
    "    def _merge_close_spans(self, spans, y_threshold=1.5):\n",
    "        merged = []\n",
    "        current = None\n",
    "        for span in sorted(spans, key=lambda x: (x['page'], x['y_pos'])):\n",
    "            if current and abs(current['y_pos'] - span['y_pos']) < y_threshold and current['page'] == span['page']:\n",
    "                current['text'] += ' ' + span['text']\n",
    "            else:\n",
    "                if current:\n",
    "                    merged.append(current)\n",
    "                current = span.copy()\n",
    "        if current:\n",
    "            merged.append(current)\n",
    "        return merged\n",
    "\n",
    "    def _extract_candidate_headings(self, doc, font_stats):\n",
    "        candidates = []\n",
    "        common_fonts = font_stats[\"common_fonts\"]\n",
    "        spans = font_stats[\"spans\"]\n",
    "        spans_by_page = defaultdict(list)\n",
    "        for span in spans:\n",
    "            spans_by_page[span[\"page\"]].append(span)\n",
    "        for span in spans:\n",
    "            text = span[\"text\"]\n",
    "            size = span[\"size\"]\n",
    "            is_bold = span[\"bold\"]\n",
    "            page = span[\"page\"]\n",
    "            if len(text.split()) > self.max_heading_words:\n",
    "                continue\n",
    "\n",
    "            features = {\n",
    "                \"font_size\": size,\n",
    "                \"is_bold\": is_bold,\n",
    "                \"word_count\": len(text.split()),\n",
    "                \"has_number_prefix\": bool(re.match(r'^\\d+(\\.\\d+)*\\.?\\s', text)),\n",
    "                \"is_all_caps\": text.isupper(),\n",
    "                \"ends_with_colon\": text.endswith(':'),\n",
    "                \"at_page_top\": self._is_at_page_top(span, spans_by_page[page]),\n",
    "                \"standalone_line\": self._is_standalone(span, spans_by_page[page])\n",
    "            }\n",
    "            score = self._score_candidate(features, common_fonts)\n",
    "            if score > 0.5:\n",
    "                candidates.append({\n",
    "                    \"text\": text,\n",
    "                    \"page\": page,\n",
    "                    \"features\": features,\n",
    "                    \"score\": score,\n",
    "                    \"size\": size,\n",
    "                    \"is_bold\": is_bold,\n",
    "                    \"y_pos\": span[\"y_pos\"],\n",
    "                    \"bbox\": span[\"bbox\"],\n",
    "                })\n",
    "        return candidates\n",
    "\n",
    "    def _is_at_page_top(self, span, page_spans):\n",
    "        if not page_spans:\n",
    "            return False\n",
    "        sorted_spans = sorted(page_spans, key=lambda x: x[\"y_pos\"])\n",
    "        return span[\"y_pos\"] <= sorted_spans[0][\"y_pos\"] + 0.15 * (sorted_spans[-1][\"y_pos\"] - sorted_spans[0][\"y_pos\"])\n",
    "\n",
    "    def _is_standalone(self, span, page_spans):\n",
    "        bbox = span[\"bbox\"]\n",
    "        for other in page_spans:\n",
    "            if other == span:\n",
    "                continue\n",
    "            y_overlap = max(0, min(bbox[3], other[\"bbox\"][3]) - max(bbox[1], other[\"bbox\"][1]))\n",
    "            if y_overlap > 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _score_candidate(self, features, common_fonts):\n",
    "        score = 0\n",
    "        for i, (font_size, _) in enumerate(common_fonts[:3]):\n",
    "            if abs(features[\"font_size\"] - font_size) < 0.5:\n",
    "                score += 0.3 - (i * 0.1)\n",
    "                break\n",
    "        if features[\"is_bold\"]:\n",
    "            score += 0.2\n",
    "        if features[\"has_number_prefix\"]:\n",
    "            score += 0.3\n",
    "        if features[\"is_all_caps\"]:\n",
    "            score += 0.15\n",
    "        if features[\"ends_with_colon\"]:\n",
    "            score += 0.15\n",
    "        # No keyword-based scoring here\n",
    "        if features[\"at_page_top\"]:\n",
    "            score += 0.25\n",
    "        if features[\"standalone_line\"]:\n",
    "            score += 0.2\n",
    "        if features[\"word_count\"] > 8:\n",
    "            score -= 0.1 * (features[\"word_count\"] - 8)\n",
    "        return min(1.0, max(0.0, score))\n",
    "\n",
    "    def _classify_headings_with_nlp(self, candidates):\n",
    "        if not candidates:\n",
    "            return []\n",
    "        texts = [c[\"text\"] for c in candidates]\n",
    "        docs = list(nlp.pipe(texts, disable=[\"ner\"]))\n",
    "        filtered = []\n",
    "        for i, (candidate, doc) in enumerate(zip(candidates, docs)):\n",
    "            text = candidate[\"text\"]\n",
    "        # Must start uppercase letter\n",
    "            if not text[0].isupper():\n",
    "                continue\n",
    "        # Word count limits\n",
    "            wc = len(text.split())\n",
    "            if wc < 3 or wc > 12:\n",
    "                continue\n",
    "            has_verb = any(token.pos_ == \"VERB\" for token in doc)\n",
    "        # Penalize if too long or contains verbs strongly\n",
    "            if has_verb and wc > 5:\n",
    "                continue\n",
    "            if '.' in text:\n",
    "                continue\n",
    "            score = candidate[\"score\"]\n",
    "        # Increase score if phrase-like pattern matched\n",
    "            pos_pattern = \" \".join([token.pos_ for token in doc])\n",
    "            common_heading_patterns = [\n",
    "            \"^(DET )?(ADJ )*(NOUN|PROPN)\",  # noun phrase start\n",
    "            \"^NUM\",\n",
    "            \"^ADV ADJ\"\n",
    "        ]\n",
    "            matches_pattern = any(re.search(pattern, pos_pattern) for pattern in common_heading_patterns)\n",
    "            if matches_pattern:\n",
    "                score += 0.2\n",
    "            if score >= 0.7:\n",
    "                candidate[\"score\"] = score\n",
    "                filtered.append(candidate)\n",
    "        return filtered\n",
    "\n",
    "\n",
    "    def _assign_heading_levels(self, headings):\n",
    "        if not headings:\n",
    "            return []\n",
    "        sorted_headings = sorted(headings, key=lambda x: x[\"score\"], reverse=True)\n",
    "        sizes = [h[\"size\"] for h in sorted_headings]\n",
    "        size_counts = defaultdict(int)\n",
    "        for size in sizes:\n",
    "            size_counts[size] += 1\n",
    "        common_sizes = sorted(size_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        size_to_level = {}\n",
    "        for i, (size, _) in enumerate(common_sizes[:3]):\n",
    "            size_to_level[size] = f\"H{i+1}\"\n",
    "        default_level = \"H3\"\n",
    "        outline = []\n",
    "        for heading in sorted_headings:\n",
    "            level = size_to_level.get(heading[\"size\"], default_level)\n",
    "            if heading[\"score\"] < 0.75 and level == \"H3\":\n",
    "                continue\n",
    "            outline.append({\n",
    "                \"level\": level,\n",
    "                \"text\": heading[\"text\"],\n",
    "                \"page\": heading[\"page\"],\n",
    "                \"y_pos\": heading[\"y_pos\"]\n",
    "            })\n",
    "        return outline\n",
    "\n",
    "    def _deduplicate_outline(self, outline):\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for item in outline:\n",
    "            key = (item[\"text\"].strip().lower(), item[\"page\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                result.append(item)\n",
    "        return result\n",
    "\n",
    "\n",
    "def extract_refined_text(pdf_path, page_number, heading_y_pos, max_sentences=3):\n",
    "    \"\"\"Extract up to max_sentences sentences below heading on the page.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_number - 1]  # zero-based\n",
    "    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "    candidate_blocks = [b for b in blocks if b.get(\"bbox\") and b[\"bbox\"][1] > heading_y_pos + 2]\n",
    "    candidate_blocks = sorted(candidate_blocks, key=lambda b: b[\"bbox\"][1])\n",
    "\n",
    "    accumulated_text = \"\"\n",
    "    sentence_count = 0\n",
    "\n",
    "    for block in candidate_blocks:\n",
    "        block_text = \"\"\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for span in line.get(\"spans\", []):\n",
    "                block_text += span.get(\"text\", \"\") + \" \"\n",
    "        block_text = block_text.strip()\n",
    "\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', block_text)\n",
    "\n",
    "        for sent in sentences:\n",
    "            if sentence_count < max_sentences:\n",
    "                accumulated_text += sent + \" \"\n",
    "                sentence_count += 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if sentence_count >= max_sentences:\n",
    "            break\n",
    "\n",
    "    accumulated_text = accumulated_text.strip()\n",
    "    if sentence_count >= max_sentences:\n",
    "        accumulated_text += \"...\"\n",
    "\n",
    "    return accumulated_text if accumulated_text else \"\"\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    pdf_folder = \"input_pdfs\"  # Your PDFs folder\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    persona = \"Travel Planner\"\n",
    "    job_to_be_done = \"Plan a trip of 4 days for a group of 10 college friends.\"\n",
    "    processing_timestamp = datetime.now().isoformat()\n",
    "\n",
    "    extractor = HeadingExtractor()\n",
    "    all_sections = []\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        full_path = os.path.join(pdf_folder, pdf_file)\n",
    "        result = extractor.extract_from_pdf(full_path)\n",
    "        for section in result.get(\"outline\", []):\n",
    "            all_sections.append({\n",
    "                \"document\": pdf_file,\n",
    "                \"section_title\": section[\"text\"],\n",
    "                \"page_number\": section[\"page\"],  \n",
    "                \"y_pos\": section.get(\"y_pos\", 0)\n",
    "            })\n",
    "\n",
    "    filtered_sections = [s for s in all_sections if not is_generic_heading(s[\"section_title\"])]\n",
    "\n",
    "    if len(filtered_sections) < 10:\n",
    "        intro_sections = [s for s in all_sections if is_generic_heading(s[\"section_title\"])]\n",
    "        intro_by_doc = {}\n",
    "        for s in intro_sections:\n",
    "            if s[\"document\"] not in intro_by_doc:\n",
    "                intro_by_doc[s[\"document\"]] = s\n",
    "        filtered_sections.extend(intro_by_doc.values())\n",
    "\n",
    "    all_sections = filtered_sections\n",
    "\n",
    "    all_sections.sort(key=lambda s: (section_score(s), -s[\"page_number\"]), reverse=True)\n",
    "\n",
    "    seen_titles = set()\n",
    "    final_sections = []\n",
    "    for s in all_sections:\n",
    "        title_lower = s[\"section_title\"].strip().lower()\n",
    "        if title_lower not in seen_titles:\n",
    "            seen_titles.add(title_lower)\n",
    "            final_sections.append(s)\n",
    "        if len(final_sections) >= 10: \n",
    "            break\n",
    "\n",
    "    final_subsections = []\n",
    "    for s in final_sections:\n",
    "        refined_text = extract_refined_text(\n",
    "            os.path.join(pdf_folder, s[\"document\"]),\n",
    "            s[\"page_number\"],\n",
    "            s.get(\"y_pos\", 0)\n",
    "        )\n",
    "        final_subsections.append({\n",
    "            \"document\": s[\"document\"],\n",
    "            \"refined_text\": refined_text,\n",
    "            \"page_number\": s[\"page_number\"]\n",
    "        })\n",
    "\n",
    "    output = {\n",
    "        \"metadata\": {\n",
    "            \"input_documents\": pdf_files,\n",
    "            \"persona\": persona,\n",
    "            \"job_to_be_done\": job_to_be_done,\n",
    "            \"processing_timestamp\": processing_timestamp\n",
    "        },\n",
    "        \"extracted_sections\": [\n",
    "            {\n",
    "                \"document\": s[\"document\"],\n",
    "                \"section_title\": s[\"section_title\"],\n",
    "                \"importance_rank\": i + 1,\n",
    "                \"page_number\": s[\"page_number\"]\n",
    "            } for i, s in enumerate(final_sections)\n",
    "        ],\n",
    "        \"subsection_analysis\": final_subsections\n",
    "    }\n",
    "\n",
    "    with open(\"output_result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"‚úÖ Extraction complete. Output saved to output_result.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42c41264-3cb0-4e49-95d6-2e9a8840af0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è Downloading model for the first time (will be saved locally)...\n",
      "‚úÖ Model downloaded and cached for offline use.\n",
      "‚úÖ Job ‚Üî PDF relevance OK (best match 0.34).\n",
      "‚úÖ Persona ‚Üî Job relevance OK (similarity=0.36).\n",
      "‚úÖ Extraction complete. Output saved to output_result.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ----------------------------- #\n",
    "# Load Spacy and Transformer Models\n",
    "# ----------------------------- #\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def load_model_offline(model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Loads or caches the sentence transformer model for offline use.\"\"\"\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/sentence_transformers\")\n",
    "    model_path = os.path.join(cache_dir, model_name)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"üîπ Loading semantic model from local cache...\")\n",
    "        return SentenceTransformer(model_path)\n",
    "    else:\n",
    "        print(\"‚¨áÔ∏è Downloading model for the first time (will be saved locally)...\")\n",
    "        model = SentenceTransformer(model_name, cache_folder=cache_dir)\n",
    "        print(\"‚úÖ Model downloaded and cached for offline use.\")\n",
    "        return model\n",
    "\n",
    "semantic_model = load_model_offline()\n",
    "\n",
    "# ----------------------------- #\n",
    "# New relevance checking functions (semantic, uses actual PDF headings)\n",
    "# ----------------------------- #\n",
    "def compute_job_to_docs_similarity(job_to_be_done, all_sections, pdf_folder, top_k=10):\n",
    "    \"\"\"\n",
    "    Compute max semantic similarity between job and the extracted section titles (and short refined text)\n",
    "    Returns the maximum similarity value and a small sample of matching sections.\n",
    "    \"\"\"\n",
    "    job_text = job_to_be_done.lower().strip()\n",
    "    if not all_sections:\n",
    "        return 0.0, []\n",
    "\n",
    "    # Build candidate texts: use section titles first; if available, also include short refined text for top sections\n",
    "    candidates = []\n",
    "    for s in all_sections:\n",
    "        # include the title\n",
    "        candidates.append(f\"{s['section_title']}\")\n",
    "    # deduplicate while preserving order\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for c, s in zip(candidates, all_sections):\n",
    "        key = c.strip().lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append((c, s))\n",
    "    # limit number of candidates to speed up compute\n",
    "    deduped = deduped[:max(top_k, 20)]\n",
    "\n",
    "    # encode job and candidate texts\n",
    "    job_emb = semantic_model.encode(job_text, convert_to_tensor=True)\n",
    "    texts = [c for c, _ in deduped]\n",
    "    text_embs = semantic_model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    sims = util.cos_sim(job_emb, text_embs)[0].cpu().tolist()\n",
    "    best_idx = int(max(range(len(sims)), key=lambda i: sims[i]))\n",
    "    best_score = sims[best_idx]\n",
    "    # gather top matches\n",
    "    top_matches = sorted(\n",
    "        [(sims[i], deduped[i][0], deduped[i][1]) for i in range(len(sims))],\n",
    "        key=lambda x: x[0],\n",
    "        reverse=True\n",
    "    )[:3]\n",
    "    return float(best_score), top_matches\n",
    "\n",
    "def check_relevance(persona, job_to_be_done, all_sections, pdf_folder,\n",
    "                    doc_threshold=0.30, persona_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Returns True if:\n",
    "      - job is semantically relevant to the PDF content (section titles)\n",
    "      - persona semantically fits the job\n",
    "    Otherwise prints a friendly message and returns False.\n",
    "    \"\"\"\n",
    "    persona_clean = persona.lower().strip()\n",
    "    job_clean = job_to_be_done.lower().strip()\n",
    "\n",
    "    # 1) Check job <> document relevance\n",
    "    best_score, top_matches = compute_job_to_docs_similarity(job_clean, all_sections, pdf_folder, top_k=40)\n",
    "    if best_score < doc_threshold:\n",
    "        # print informative message with top candidate examples\n",
    "        if top_matches:\n",
    "            example_texts = \", \".join([f\"'{m[1]}'({m[0]:.2f})\" for m in top_matches])\n",
    "            print(f\"‚ùå The job appears unrelated to the content in the provided PDFs (best match {best_score:.2f}).\")\n",
    "            print(f\"   Top section candidates: {example_texts}\")\n",
    "        else:\n",
    "            print(\"‚ùå The job appears unrelated to the content in the provided PDFs (no headings found).\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"‚úÖ Job ‚Üî PDF relevance OK (best match {best_score:.2f}).\")\n",
    "\n",
    "    # 2) Check persona <> job relevance\n",
    "    # compute similarity between persona label and the job text\n",
    "    persona_emb = semantic_model.encode(persona_clean, convert_to_tensor=True)\n",
    "    job_emb = semantic_model.encode(job_clean, convert_to_tensor=True)\n",
    "    persona_job_sim = util.cos_sim(persona_emb, job_emb).item()\n",
    "\n",
    "    if persona_job_sim < persona_threshold:\n",
    "        print(f\"‚ùå The job '{job_to_be_done}' seems to not match persona '{persona}' (similarity={persona_job_sim:.2f}).\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"‚úÖ Persona ‚Üî Job relevance OK (similarity={persona_job_sim:.2f}).\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# ----------------------------- #\n",
    "# PDF Heading Extraction Logic (unchanged except minor tidy)\n",
    "# ----------------------------- #\n",
    "class HeadingExtractor:\n",
    "    def __init__(self):\n",
    "        self.heading_keywords = [\n",
    "            \"introduction\", \"abstract\", \"conclusion\", \"references\",\n",
    "            \"methodology\", \"results\", \"discussion\", \"background\",\n",
    "            \"chapter\", \"section\", \"appendix\"\n",
    "        ]\n",
    "        self.min_font_occurrences = 3\n",
    "        self.max_heading_words = 12\n",
    "\n",
    "    def extract_from_pdf(self, pdf_path):\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            toc = doc.get_toc()\n",
    "            if toc and len(toc) > 2:\n",
    "                return self._extract_from_toc(doc, toc)\n",
    "            return self._extract_with_nlp(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting from PDF '{pdf_path}': {e}\")\n",
    "            return {\"title\": \"Unknown\", \"outline\": []}\n",
    "\n",
    "    def _extract_from_toc(self, doc, toc):\n",
    "        title = doc.metadata.get(\"title\", \"\")\n",
    "        outline = []\n",
    "        for item in toc:\n",
    "            level, heading_text, page = item\n",
    "            if level <= 3:\n",
    "                outline.append({\n",
    "                    \"level\": f\"H{level}\",\n",
    "                    \"text\": heading_text,\n",
    "                    \"page\": page-1\n",
    "                })\n",
    "        if not title and outline:\n",
    "            title = outline[0][\"text\"]\n",
    "        return {\"title\": title, \"outline\": sorted(outline, key=lambda x: x[\"page\"])}\n",
    "\n",
    "    def _extract_with_nlp(self, doc):\n",
    "        font_stats = self._analyze_fonts(doc)\n",
    "        font_stats[\"spans\"] = self._merge_close_spans(font_stats[\"spans\"])\n",
    "        candidates = self._extract_candidate_headings(doc, font_stats)\n",
    "        headings = self._classify_headings_with_nlp(candidates)\n",
    "        outline = self._assign_heading_levels(headings)\n",
    "        outline = self._deduplicate_outline(outline)\n",
    "        outline.sort(key=lambda x: x[\"page\"])\n",
    "        title = doc.metadata.get(\"title\", \"\")\n",
    "        if not title and outline:\n",
    "            title = outline[0][\"text\"]\n",
    "        return {\"title\": title, \"outline\": outline}\n",
    "\n",
    "    def _analyze_fonts(self, doc):\n",
    "        fonts = defaultdict(int)\n",
    "        font_details = []\n",
    "        for page_num, page in enumerate(doc):\n",
    "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "            for block in blocks:\n",
    "                for line in block.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        size = round(span.get(\"size\", 0), 1)\n",
    "                        is_bold = span.get(\"flags\", 0) & 2 > 0\n",
    "                        text = span.get(\"text\", \"\").strip()\n",
    "                        if not text:\n",
    "                            continue\n",
    "                        fonts[size] += 1\n",
    "                        font_details.append({\n",
    "                            \"text\": text,\n",
    "                            \"size\": size,\n",
    "                            \"bold\": is_bold,\n",
    "                            \"page\": page_num,\n",
    "                            \"y_pos\": span.get(\"bbox\")[1],\n",
    "                            \"bbox\": span.get(\"bbox\")\n",
    "                        })\n",
    "        common_fonts = sorted(\n",
    "            [(size, count) for size, count in fonts.items()\n",
    "             if count >= self.min_font_occurrences],\n",
    "            key=lambda x: x[0], reverse=True\n",
    "        )\n",
    "        return {\"common_fonts\": common_fonts, \"spans\": font_details}\n",
    "\n",
    "    def _merge_close_spans(self, spans, y_threshold=1.5):\n",
    "        merged = []\n",
    "        current = None\n",
    "        for span in sorted(spans, key=lambda x: (x['page'], x['y_pos'])):\n",
    "            if current and abs(current['y_pos'] - span['y_pos']) < y_threshold and current['page'] == span['page']:\n",
    "                current['text'] += ' ' + span['text']\n",
    "            else:\n",
    "                if current:\n",
    "                    merged.append(current)\n",
    "                current = span.copy()\n",
    "        if current:\n",
    "            merged.append(current)\n",
    "        return merged\n",
    "\n",
    "    def _extract_candidate_headings(self, doc, font_stats):\n",
    "        candidates = []\n",
    "        common_fonts = font_stats[\"common_fonts\"]\n",
    "        spans = font_stats[\"spans\"]\n",
    "        spans_by_page = defaultdict(list)\n",
    "        for span in spans:\n",
    "            spans_by_page[span[\"page\"]].append(span)\n",
    "        for span in spans:\n",
    "            text = span[\"text\"]\n",
    "            size = span[\"size\"]\n",
    "            is_bold = span[\"bold\"]\n",
    "            page = span[\"page\"]\n",
    "            if len(text.split()) > self.max_heading_words:\n",
    "                continue\n",
    "            features = {\n",
    "                \"font_size\": size,\n",
    "                \"is_bold\": is_bold,\n",
    "                \"word_count\": len(text.split()),\n",
    "                \"has_number_prefix\": bool(re.match(r'^\\d+(\\.\\d+)*\\.?\\s', text)),\n",
    "                \"is_all_caps\": text.isupper(),\n",
    "                \"ends_with_colon\": text.endswith(':'),\n",
    "                \"has_heading_keyword\": any(keyword in text.lower() for keyword in self.heading_keywords),\n",
    "                \"at_page_top\": self._is_at_page_top(span, spans_by_page[page]),\n",
    "                \"standalone_line\": self._is_standalone(span, spans_by_page[page])\n",
    "            }\n",
    "            score = self._score_candidate(features, common_fonts)\n",
    "            if score > 0.5:\n",
    "                candidates.append({\n",
    "                    \"text\": text,\n",
    "                    \"page\": page,\n",
    "                    \"features\": features,\n",
    "                    \"score\": score,\n",
    "                    \"size\": size,\n",
    "                    \"is_bold\": is_bold,\n",
    "                    \"y_pos\": span[\"y_pos\"],\n",
    "                    \"bbox\": span[\"bbox\"],\n",
    "                })\n",
    "        return candidates\n",
    "\n",
    "    def _is_at_page_top(self, span, page_spans):\n",
    "        if not page_spans:\n",
    "            return False\n",
    "        sorted_spans = sorted(page_spans, key=lambda x: x[\"y_pos\"])\n",
    "        return span[\"y_pos\"] <= sorted_spans[0][\"y_pos\"] + 0.15 * (sorted_spans[-1][\"y_pos\"] - sorted_spans[0][\"y_pos\"])\n",
    "\n",
    "    def _is_standalone(self, span, page_spans):\n",
    "        bbox = span[\"bbox\"]\n",
    "        for other in page_spans:\n",
    "            if other == span:\n",
    "                continue\n",
    "            y_overlap = max(0, min(bbox[3], other[\"bbox\"][3]) - max(bbox[1], other[\"bbox\"][1]))\n",
    "            if y_overlap > 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _score_candidate(self, features, common_fonts):\n",
    "        score = 0\n",
    "        for i, (font_size, _) in enumerate(common_fonts[:3]):\n",
    "            if abs(features[\"font_size\"] - font_size) < 0.5:\n",
    "                score += 0.3 - (i * 0.1)\n",
    "                break\n",
    "        if features[\"is_bold\"]:\n",
    "            score += 0.2\n",
    "        if features[\"has_number_prefix\"]:\n",
    "            score += 0.3\n",
    "        if features[\"is_all_caps\"]:\n",
    "            score += 0.15\n",
    "        if features[\"ends_with_colon\"]:\n",
    "            score += 0.15\n",
    "        if features[\"has_heading_keyword\"]:\n",
    "            score += 0.2\n",
    "        if features[\"at_page_top\"]:\n",
    "            score += 0.25\n",
    "        if features[\"standalone_line\"]:\n",
    "            score += 0.2\n",
    "        if features[\"word_count\"] > 8:\n",
    "            score -= 0.1 * (features[\"word_count\"] - 8)\n",
    "        return min(1.0, max(0.0, score))\n",
    "\n",
    "    def _classify_headings_with_nlp(self, candidates):\n",
    "        if not candidates:\n",
    "            return []\n",
    "        texts = [c[\"text\"] for c in candidates]\n",
    "        docs = list(nlp.pipe(texts, disable=[\"ner\"]))\n",
    "        for i, (candidate, doc) in enumerate(zip(candidates, docs)):\n",
    "            has_verb = any(token.pos_ == \"VERB\" for token in doc)\n",
    "            pos_pattern = \" \".join([token.pos_ for token in doc])\n",
    "            common_heading_patterns = [\n",
    "                \"^(DET )?(ADJ )*(NOUN|PROPN)\",\n",
    "                \"^NUM\",\n",
    "                \"^(VERB|AUX)\",\n",
    "                \"^ADV ADJ\"\n",
    "            ]\n",
    "            matches_pattern = any(re.search(pattern, pos_pattern) for pattern in common_heading_patterns)\n",
    "            if matches_pattern:\n",
    "                candidates[i][\"score\"] += 0.2\n",
    "            if has_verb and len(doc) > 5:\n",
    "                candidates[i][\"score\"] -= 0.25\n",
    "            if '.' in candidate[\"text\"] or len(candidate[\"text\"].split()) > 12:\n",
    "                candidates[i][\"score\"] -= 0.3\n",
    "        return [c for c in candidates if c[\"score\"] >= 0.6]\n",
    "\n",
    "    def _assign_heading_levels(self, headings):\n",
    "        if not headings:\n",
    "            return []\n",
    "        sorted_headings = sorted(headings, key=lambda x: x[\"score\"], reverse=True)\n",
    "        sizes = [h[\"size\"] for h in sorted_headings]\n",
    "        size_counts = defaultdict(int)\n",
    "        for size in sizes:\n",
    "            size_counts[size] += 1\n",
    "        common_sizes = sorted(size_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        size_to_level = {}\n",
    "        for i, (size, _) in enumerate(common_sizes[:3]):\n",
    "            size_to_level[size] = f\"H{i+1}\"\n",
    "        default_level = \"H3\"\n",
    "        outline = []\n",
    "        for heading in sorted_headings:\n",
    "            level = size_to_level.get(heading[\"size\"], default_level)\n",
    "            if heading[\"score\"] < 0.75 and level == \"H3\":\n",
    "                continue\n",
    "            outline.append({\n",
    "                \"level\": level,\n",
    "                \"text\": heading[\"text\"],\n",
    "                \"page\": heading[\"page\"],\n",
    "                \"y_pos\": heading[\"y_pos\"]\n",
    "            })\n",
    "        return outline\n",
    "\n",
    "    def _deduplicate_outline(self, outline):\n",
    "        seen = set()\n",
    "        result = []\n",
    "        for item in outline:\n",
    "            key = (item[\"text\"].strip().lower(), item[\"page\"])\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                result.append(item)\n",
    "        return result\n",
    "\n",
    "# ----------------------------- #\n",
    "# Supporting Functions\n",
    "# ----------------------------- #\n",
    "def extract_refined_text(pdf_path, page_number, heading_y_pos, max_sentences=3):\n",
    "    \"\"\"Extracts text below the heading.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[page_number]\n",
    "    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "    candidate_blocks = [b for b in blocks if b.get(\"bbox\") and b[\"bbox\"][1] > heading_y_pos + 2]\n",
    "    candidate_blocks = sorted(candidate_blocks, key=lambda b: b[\"bbox\"][1])\n",
    "    accumulated_text, sentence_count = \"\", 0\n",
    "    for block in candidate_blocks:\n",
    "        text = \" \".join(span.get(\"text\", \"\") for line in block.get(\"lines\", []) for span in line.get(\"spans\", []))\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "        for sent in sentences:\n",
    "            if sentence_count < max_sentences:\n",
    "                accumulated_text += sent + \" \"\n",
    "                sentence_count += 1\n",
    "            else:\n",
    "                break\n",
    "        if sentence_count >= max_sentences:\n",
    "            break\n",
    "    accumulated_text = accumulated_text.strip()\n",
    "    if sentence_count >= max_sentences:\n",
    "        accumulated_text += \"...\"\n",
    "    return accumulated_text if accumulated_text else \"\"\n",
    "\n",
    "def is_generic_heading(title):\n",
    "    return title.strip().lower() in {\"introduction\", \"summary\", \"overview\", \"contents\"}\n",
    "\n",
    "def section_score(section):\n",
    "    title = section[\"section_title\"].lower()\n",
    "    score = 0\n",
    "    keywords = [\"guide\", \"adventure\", \"tips\", \"experience\", \"travel\", \"nightlife\", \"culinary\", \"history\", \"restaurants\", \"hotels\", \"things to do\"]\n",
    "    for keyword in keywords:\n",
    "        if keyword in title:\n",
    "            score += 3\n",
    "    if len(title.split()) > 3:\n",
    "        score += 1\n",
    "    return score\n",
    "\n",
    "# ----------------------------- #\n",
    "# MAIN FUNCTION\n",
    "# ----------------------------- #\n",
    "def main():\n",
    "    pdf_folder = \"input_pdfs\"\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "    persona = \"Travel Planner\"\n",
    "    job_to_be_done = \"make a exam preparation plan\"\n",
    "    processing_timestamp = datetime.now().isoformat()\n",
    "\n",
    "    extractor = HeadingExtractor()\n",
    "    all_sections = []\n",
    "\n",
    "    # Extract headings from PDFs first (we use those to decide relevance)\n",
    "    for pdf_file in pdf_files:\n",
    "        result = extractor.extract_from_pdf(os.path.join(pdf_folder, pdf_file))\n",
    "        for section in result.get(\"outline\", []):\n",
    "            all_sections.append({\n",
    "                \"document\": pdf_file,\n",
    "                \"section_title\": section[\"text\"],\n",
    "                \"page_number\": section[\"page\"],\n",
    "                \"y_pos\": section.get(\"y_pos\", 0)\n",
    "            })\n",
    "\n",
    "    if not all_sections:\n",
    "        print(\"‚ö†Ô∏è No sections found in provided PDFs. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # Check persona-job-doc relevance (prints error messages when failing)\n",
    "    if not check_relevance(persona, job_to_be_done, all_sections, pdf_folder):\n",
    "        print(\"‚ùå Aborting PDF extraction due to irrelevance.\")\n",
    "        return\n",
    "\n",
    "    # proceed with existing filtering and selection logic\n",
    "    filtered_sections = [s for s in all_sections if not is_generic_heading(s[\"section_title\"])]\n",
    "    if len(filtered_sections) < 5:\n",
    "        intro_sections = [s for s in all_sections if is_generic_heading(s[\"section_title\"])]\n",
    "        intro_by_doc = {s[\"document\"]: s for s in intro_sections}\n",
    "        filtered_sections.extend(intro_by_doc.values())\n",
    "\n",
    "    all_sections = filtered_sections\n",
    "    all_sections.sort(key=lambda s: (section_score(s), -s[\"page_number\"]), reverse=True)\n",
    "\n",
    "    seen_titles = set()\n",
    "    final_sections = []\n",
    "    for s in all_sections:\n",
    "        title_lower = s[\"section_title\"].strip().lower()\n",
    "        if title_lower not in seen_titles:\n",
    "            seen_titles.add(title_lower)\n",
    "            final_sections.append(s)\n",
    "        if len(final_sections) >= 5:\n",
    "            break\n",
    "\n",
    "    final_subsections = []\n",
    "    for s in final_sections:\n",
    "        refined_text = extract_refined_text(os.path.join(pdf_folder, s[\"document\"]), s[\"page_number\"], s.get(\"y_pos\", 0))\n",
    "        final_subsections.append({\n",
    "            \"document\": s[\"document\"],\n",
    "            \"refined_text\": refined_text,\n",
    "            \"page_number\": s[\"page_number\"]\n",
    "        })\n",
    "\n",
    "    output = {\n",
    "        \"metadata\": {\n",
    "            \"input_documents\": pdf_files,\n",
    "            \"persona\": persona,\n",
    "            \"job_to_be_done\": job_to_be_done,\n",
    "            \"processing_timestamp\": processing_timestamp\n",
    "        },\n",
    "        \"extracted_sections\": [\n",
    "            {\n",
    "                \"document\": s[\"document\"],\n",
    "                \"section_title\": s[\"section_title\"],\n",
    "                \"importance_rank\": i + 1,\n",
    "                \"page_number\": s[\"page_number\"]\n",
    "            } for i, s in enumerate(final_sections)\n",
    "        ],\n",
    "        \"subsection_analysis\": final_subsections\n",
    "    }\n",
    "\n",
    "    with open(\"output_result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"‚úÖ Extraction complete. Output saved to output_result.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97645920-7b4f-47f9-b8d8-5924896b537b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
